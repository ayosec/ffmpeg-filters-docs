# frozen_string_literal: true

require "digest/sha2"
require "net/http"
require "nokogiri"
require "tempfile"

require_relative "html_adapter"
require_relative "saxonb_cmd"

module FFDocs::SourceDocs

  PATCHES_DIR = Pathname.new(File.expand_path("../patches", __FILE__))

  DTD_MUTEX = Mutex.new

  class SourceNotFound < StandardError; end
  class XMLTransformFailed < StandardError; end

  Group = Struct.new(
    :release,
    :media_type,
    :component,
    :items,
    keyword_init: true
  )

  Item = Struct.new(
    :group,
    :name,
    :first_version,
    :html,
    :description,
    keyword_init: true,
  ) do
    def inspect
      %[Item[#{name} @ #{group.media_type}/#{group.component}]]
    end
  end

  class Collection

    SOURCE_FILE =  "doc/filters.texi"

    attr_reader :storage, :groups, :anchors

    def initialize(options, storage, release)
      @html_adapter = HTMLAdapter.new(options)
      @storage = storage
      @release = release

      @groups = []
      @anchors = {}

      texinfo = storage.get_file(release, SOURCE_FILE)
      raise SourceNotFound if texinfo.nil?

      # Apply local patches, if any.
      patch = PATCHES_DIR.join("#{release.version}.patch")
      if patch.exist?
        tmpfile = Tempfile.new("ffdocs-source")
        tmpfile.write(texinfo)
        tmpfile.close

        diff = patch.read

        command = %w(patch --batch --output=- --input=-)
        command << tmpfile.path

        IO.popen(command, "r+") do |io|
          io.write(diff)
          io.close_write
          texinfo = io.read
        end
      end

      ::FFDocs.log.info "Converting texinfo source to XML for #{release.tag} ..."
      xml = IO.popen(%w(makeinfo --xml --no-split --output=-), "r+") do |io|
        io.write(texinfo)
        io.close_write
        io.read
      end

      xml = cache_dtd(xml, storage)

      ::FFDocs.log.info "Parsing source for #{@release.version} ..."
      parse_xml(release, Nokogiri::XML.parse(xml))
    end

    # Extract groups and items from the XML document.
    private def parse_xml(release, xml)
      # Adjust <anchor> references.
      xml.search("anchor").each do |anchor|
        name = anchor["name"]
        label = anchor.inner_text

        elem = anchor
        while elem = elem.next_element
          break if elem.name != "anchor"
        end

        # If <anchor> is at the end, and the name is different to the content,
        # try to apply the reference to the next element of the parent.
        if elem.nil? && name != label
          elem = anchor.parent
          while elem.next_element.nil?
            elem = elem.parent
          end

          elem = elem.next_element
        end

        if elem
          elem.set_attribute("ref-name", name)
        end
      end

      # Convert the XML generated by makeinfo to HTML.
      doc = Nokogiri::XML.parse(xml_to_html(xml))

      # Split sections and collect items for each component/media-type group.
      doc.search(":root > group").each do |group|
        if group["title"] =~ /\A(.*) (Filters|Sinks|Sources)\Z/
          @groups << parse_group(release, group, $1, $2)
        end
      end
    end

    # Parse a <chapter> in the XML to build a group.
    private def parse_group(release, elem, media_type, component)
      group = Group.new(release: release, media_type: media_type, component: component)

      group.items = elem.search("> section").
        select {|section| section["data-title"] != "Examples" }.
        flat_map \
      do |section|
        sectiontitle = section["data-title"].split(",")

        html_data = @html_adapter.process(section)

        # Use the first sentence of the section as the description.
        #
        # If the <section> contains multiple filters, it may use different
        # paragraphs to describe each one. To detect those cases, we search
        # a <para> containing a <code> with the filter name.
        #
        # Any URL between parenthesis will be removed.
        base_desc = [
          section
            .at("p")
            .inner_text
            .gsub(/\s*\(https?:.*?\)/, "")
            .split(".", 2)
            .first
            .gsub(/\s+/, " "),
          ".",
        ].join

        sectiontitle.map(&:strip).map do |name|
          desc = base_desc

          if sectiontitle.size > 1
            elem = section.search("p > code").find {|elem| elem.inner_text == name }
            if elem
              desc = [ elem.parent.inner_text.split(".", 2).first, "." ].join
            end
          end

          item = Item.new(
            group: group,
            name: name,
            html: html_data.html,
            description: desc
          )

          # Store references found in this item
          html_data.anchors.each do |anchor|
            @anchors[anchor] = item
          end

          item
        end
      end

      group
    end

    # Convert the XML generated by `makeinfo` to the HTML that will be included
    # in the generated pages. It requires the `saxonb-xslt` program.
    private def xml_to_html(doc)
      xslt_source = File.expand_path("../section_content.xsl", __FILE__)

      cmd = [ SaxonbCmd.get_path, "-strip:all", "-xsl:#{xslt_source}", "-" ]

      child = IO.popen(cmd, "r+")
      child.write(doc.to_xml)
      child.close_write
      html = child.read

      Process.waitpid(child.pid)
      if not $?.success?
        raise XMLTransformFailed
      end

      html
    end

    # Replace the URLs for the DTD to use a cached file.
    private def cache_dtd(xml, storage)
      xml.sub(%r[<!DOCTYPE .*?>]) do |m|
        m.gsub(/https?:[^"]+/) do |url|
          cached = storage.dtd_file(url)

          DTD_MUTEX.synchronize do
            if not cached.exist?
              url = url.sub(/^http:/, "https:")
              ::FFDocs.log.info "Downloading DTD for #{url} ..."

              case Net::HTTP.get_response(URI(url))
              in Net::HTTPSuccess => response
                cached.write(response.body)
              in _ => failure
                ::FFDocs.log.error "Failed request: #{url}: #{failure}"
              end
            end
          end

          cached.to_s
        end
      end
    end


  end

end
